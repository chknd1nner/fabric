{"cells":[{"cell_type":"code","source":["# Configuration and Common Imports\n","\n","# Set datetime handling configuration for Spark 3.0+\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n","\n","# Common imports\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, concat, lit, when, year, date_format\n","from pyspark.sql.functions import to_date, expr, substring, trim, regexp_replace\n","from pyspark.sql.types import StringType, IntegerType, DateType\n","import logging\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","# Current notebook version\n","logger.info(\"Starting Confirm asset data transformation - v1.0\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"865c6c3d-7057-497b-a1d6-9b8d68843664"},{"cell_type":"code","source":["# Reference Tables (Cached)\n","\n","# Load and cache frequently used small reference tables\n","logger.info(\"Loading and caching reference tables\")\n","\n","# Feature Type reference table\n","feature_types_df = spark.sql(\"\"\"\n","    SELECT feature_type_code, feature_type_name \n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_feature_type\n","\"\"\").cache()\n","\n","# Suburbs/Area reference table\n","suburbs_df = spark.sql(\"\"\"\n","    SELECT area_code, area_name \n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_area\n","\"\"\").cache()\n","\n","# Park name reference table (from CAMS_FEAT_ATTRIB_TYPE)\n","park_names_df = spark.sql(\"\"\"\n","    SELECT site_code, plot_number, feat_attrib_notes\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_feat_attrib_type\n","    WHERE attrib_type_code = 'PKAN'\n","\"\"\").cache()\n","\n","# Observation type reference table\n","obs_type_df = spark.sql(\"\"\"\n","    SELECT observe_type_key, observe_type_code, observe_type_name, obs_parm_code\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_observe_type\n","\"\"\").cache()\n","\n","# Observation parameter reference table\n","obs_param_df = spark.sql(\"\"\"\n","    SELECT obs_parm_code, obs_parm_name\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_observe_parameter\n","\"\"\").cache()\n","\n","# Observation parameter options reference table\n","obs_param_options_df = spark.sql(\"\"\"\n","    SELECT obs_parm_code, obs_parm_opt_code, obs_parm_opt_name\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_observe_parm_opt\n","\"\"\").cache()\n","\n","# Job type reference table\n","job_type_df = spark.sql(\"\"\"\n","    SELECT job_type_key, job_type_code, job_type_name\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_job_type\n","\"\"\").cache()\n","\n","# Job status reference table\n","job_status_df = spark.sql(\"\"\"\n","    SELECT status_code, status_name\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_job_status\n","\"\"\").cache()\n","\n","# Register all reference tables as temp views for SQL operations\n","feature_types_df.createOrReplaceTempView(\"feature_types\")\n","suburbs_df.createOrReplaceTempView(\"suburbs\")\n","park_names_df.createOrReplaceTempView(\"park_names\")\n","obs_type_df.createOrReplaceTempView(\"obs_type\")\n","obs_param_df.createOrReplaceTempView(\"obs_param\")\n","obs_param_options_df.createOrReplaceTempView(\"obs_param_options\")\n","job_type_df.createOrReplaceTempView(\"job_type\")\n","job_status_df.createOrReplaceTempView(\"job_status\")\n","\n","logger.info(\"Reference tables cached and registered as temp views\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fced6eab-6920-4c1f-9d9c-c302009b4b2e"},{"cell_type":"code","source":["# Core Filtering DataFrames (South Parks and All Parks)\n","\n","# Create and cache the South Parks filter DataFrame\n","logger.info(\"Creating South Parks filtered DataFrame\")\n","\n","south_parks_df = spark.sql(\"\"\"\n","    SELECT site_code, plot_number, central_asset_id\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_feature\n","    WHERE lower(feature_type_code) LIKE 'pk%' \n","    AND lower(contract_area_code) != 'pkn'\n"," \"\"\").cache()\n","\n","# Create and cache the general Park filter DataFrame\n","logger.info(\"Creating Park filter DataFrame for all parks\")\n","\n","park_filter_df = spark.sql(\"\"\"\n","    SELECT site_code, plot_number, central_asset_id\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_feature\n","    WHERE lower(feature_type_code) LIKE 'pk%'\n","\"\"\").cache()\n","\n","# Register as temp views for SQL operations\n","south_parks_df.createOrReplaceTempView(\"south_parks\")\n","park_filter_df.createOrReplaceTempView(\"park_filter\")\n","\n","logger.info(\"Core filtering DataFrames created and cached\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"73fb5e98-3a3f-4ca1-8c9c-9cf60a52411d"},{"cell_type":"code","source":["# Parks Table Transformation\n","\n","logger.info(\"Creating Parks table\")\n","\n","parks_df = spark.sql(\"\"\"\n","    SELECT \n","        f.site_code AS `Site code`,\n","        f.plot_number AS `Plot No.`,\n","        f.feature_deadflag AS `Dead flag`,\n","        f.contract_area_code AS `Contract area code`,\n","        f.feature_type_code AS `Park type code`,\n","        f.central_asset_id AS `Central asset ID`,\n","        f.feature_start_date AS `Park added`,\n","        CASE \n","            WHEN lower(f.contract_area_code) = 'pkn' THEN 'North'\n","            ELSE 'South' \n","        END AS `Service area`,\n","        ft.feature_type_name AS `Park type`,\n","        pn.feat_attrib_notes AS `Park name`,\n","        s.area_name AS `Suburb`,\n","        concat(f.site_code, '-', cast(f.plot_number as string)) AS `Site and plot`,\n","        year(f.feature_start_date) AS `Year added`\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_feature f\n","    LEFT JOIN feature_types ft ON f.feature_type_code = ft.feature_type_code\n","    LEFT JOIN park_names pn ON f.site_code = pn.site_code AND f.plot_number = pn.plot_number\n","    LEFT JOIN suburbs s ON f.area_code = s.area_code\n","    WHERE lower(f.feature_type_code) LIKE 'pk%'\n","\"\"\")\n","\n","# Display preview for debugging\n","display(parks_df.limit(10))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"id":"90a9cef6-21d0-4c46-9fdd-720295b27368"},{"cell_type":"code","source":["# Batch and Route Information\n","\n","logger.info(\"Creating Route and Batch reference DataFrames\")\n","\n","# Route information\n","route_df = spark.sql(\"\"\"\n","    SELECT insp_route_code, insp_route_name, insp_type_code\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_inspection_route\n","    WHERE insp_type_code = 'PKPA'\n","\"\"\").cache()\n","\n","# Register the DataFrame as a temp view\n","route_df.createOrReplaceTempView(\"route\")\n","\n","# Batch information with route joined - now use the registered view name\n","batch_df = spark.sql(\"\"\"\n","    SELECT \n","        b.insp_batch_no, \n","        b.insp_route_code, \n","        b.insp_create_date,\n","        r.insp_route_name\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_inspection_batch b\n","    JOIN route r ON b.insp_route_code = r.insp_route_code\n","\"\"\").cache()\n","\n","# Register batch as a temp view too\n","batch_df.createOrReplaceTempView(\"batch\")\n","\n","logger.info(\"Route and Batch reference DataFrames created\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"35d05ab6-8ed3-4602-b4d7-ee2818c4e831"},{"cell_type":"code","source":["# Inspection Table Transformation\n","\n","logger.info(\"Creating Inspection table\")\n","\n","inspection_df = spark.sql(\"\"\"\n","    SELECT \n","        i.insp_batch_no AS `Batch no.`,\n","        i.site_code AS `Site code`,\n","        i.plot_number AS `Plot no.`,\n","        i.feature_insp_date AS `Inspection datetime`,\n","        b.insp_route_code AS `Route code`,\n","        b.insp_route_name AS `Route name`,\n","        to_date(i.feature_insp_date) AS `Inspection date`,\n","        concat(i.site_code, '-', cast(i.plot_number as string)) AS `Site and Plot`,\n","        concat(cast(i.insp_batch_no as string), '-', i.site_code, '-', cast(i.plot_number as string)) AS `Batch site plot`\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_inspection_feature i\n","    JOIN batch b ON i.insp_batch_no = b.insp_batch_no\n","    JOIN south_parks sp ON i.site_code = sp.site_code AND i.plot_number = sp.plot_number\n","\"\"\")\n","\n","# Display preview for debugging\n","display(inspection_df.limit(10))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"87f33713-2a51-4942-aa73-d0659300d3b9"},{"cell_type":"code","source":["# Observations Table Transformation\n","\n","logger.info(\"Creating Observations table\")\n","\n","# Join obs_type with obs_param first\n","obs_type_with_param_df = spark.sql(\"\"\"\n","    SELECT ot.*, op.obs_parm_name\n","    FROM obs_type ot\n","    JOIN obs_param op ON ot.obs_parm_code = op.obs_parm_code\n","\"\"\").cache()\n","obs_type_with_param_df.createOrReplaceTempView(\"obs_type_with_param\")\n","\n","observations_df = spark.sql(\"\"\"\n","    SELECT \n","        ic.insp_batch_no AS `Batch No.`,\n","        ic.site_code AS `Site code`,\n","        ic.plot_number AS `Plot No.`,\n","        ic.grade_code AS `Grade code`,\n","        ic.condition_notes AS `Condition notes`,\n","        ic.survey_obs_value AS `Obs value`,\n","        b.insp_create_date AS `Inspection datetime`,\n","        ot.observe_type_code AS `Obs type code`,\n","        ot.observe_type_name AS `Obs type`,\n","        opo.obs_parm_opt_name AS `Grade`,\n","        CASE \n","            WHEN ot.observe_type_name LIKE 'Cleans%' THEN 'Cleansing'\n","            WHEN ot.observe_type_name LIKE 'Gene%' THEN 'General'\n","            WHEN ot.observe_type_name LIKE 'Horti%' THEN 'Horticulture'\n","            WHEN ot.observe_type_name LIKE 'Infrast%' THEN 'Infrastructure'\n","            WHEN ot.observe_type_name LIKE 'Tur%' THEN 'Turf'\n","            ELSE NULL\n","        END AS `Observation group`,\n","        concat(cast(ic.insp_batch_no as string), '-', ic.site_code, '-', cast(ic.plot_number as string)) AS `Batch site plot`\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_insp_condition ic\n","    JOIN batch b ON ic.insp_batch_no = b.insp_batch_no\n","    JOIN south_parks sp ON ic.site_code = sp.site_code AND ic.plot_number = sp.plot_number\n","    JOIN obs_type ot ON ic.observe_type_key = ot.observe_type_key\n","    LEFT JOIN obs_param_options opo ON ot.obs_parm_code = opo.obs_parm_code AND ic.grade_code = opo.obs_parm_opt_code\n","\"\"\")\n","\n","# Display preview for debugging\n","display(observations_df.limit(10))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"3fdeec3e-dfc8-4c56-be7d-cdac140a6cfc"},{"cell_type":"code","source":["# Job Table Transformation\n","\n","logger.info(\"Creating Jobs table\")\n","\n","# First create job status log reference dataframes\n","job_status_log_current_df = spark.sql(\"\"\"\n","    SELECT job_number, job_log_number, allocated_officer, status_code\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_job_status_log\n","\"\"\").cache()\n","job_status_log_current_df.createOrReplaceTempView(\"job_status_log\")\n","\n","job_status_log_when_logged_df = spark.sql(\"\"\"\n","    SELECT job_number, login_name AS `Logged by`\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_job_status_log\n","    WHERE job_log_number = 1\n","\"\"\").cache()\n","job_status_log_when_logged_df.createOrReplaceTempView(\"job_status_log_when_logged\")\n","\n","# Contract and contractor\n","contractor_df = spark.sql(\"\"\"\n","    SELECT contractor_code, contractor_name\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_contractor\n","\"\"\").cache()\n","contractor_df.createOrReplaceTempView(\"contractor\")\n","\n","contract_df = spark.sql(\"\"\"\n","    SELECT c.contract_code, c.contract_name, c.contractor_code, co.contractor_name\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_contract c\n","    JOIN contractor co ON c.contractor_code = co.contractor_code\n","\"\"\").cache()\n","contract_df.createOrReplaceTempView(\"contract\")\n","\n","# Action officer\n","action_officer_df = spark.sql(\"\"\"\n","    SELECT officer_code, officer_name\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_action_officer\n","\"\"\").cache()\n","action_officer_df.createOrReplaceTempView(\"action_officer\")\n","\n","# Now create the jobs table\n","jobs_df = spark.sql(\"\"\"\n","    SELECT \n","        j.job_number AS `Job number`,\n","        j.site_code AS `Site code`,\n","        j.plot_number AS `Plot number`,\n","        j.job_entry_date AS `Job entry datetime`,\n","        j.job_notes AS `Job note`,\n","        j.job_location AS `Job location`,\n","        j.job_status_flag AS `Job status flag`,\n","        j.actual_comp_date AS `Job actual completion datetime`,\n","        j.actual_start_date AS `Job start datetime`,\n","        j.target_comp_date AS `Job target completion datetime`,\n","        j.parent_job_number AS `Parent job number`,\n","        jt.job_type_name AS `Job type name`,\n","        jsl.allocated_officer AS `Allocated officer code`,\n","        jsl.status_code AS `Job status code`,\n","        js.status_name AS `Job status`,\n","        c.contract_name AS `Contract name`,\n","        c.contractor_name AS `Contractor name`,\n","        ao.officer_name AS `Allocated officer name`,\n","        jsl_log.`Logged by`,\n","        to_date(j.actual_comp_date) AS `Job complete date`,\n","        to_date(j.job_entry_date) AS `Job created date`,\n","        CASE\n","            WHEN j.actual_comp_date IS NULL THEN j.job_entry_date\n","            ELSE j.actual_comp_date\n","        END AS `last updated`,\n","        concat(j.site_code, '-', cast(j.plot_number as string)) AS `Site and plot`\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_job j\n","    JOIN park_filter pf ON j.site_code = pf.site_code AND j.plot_number = pf.plot_number\n","    LEFT JOIN job_type jt ON j.job_type_key = jt.job_type_key\n","    LEFT JOIN job_status_log jsl ON j.job_number = jsl.job_number AND j.job_log_number = jsl.job_log_number\n","    LEFT JOIN job_status js ON jsl.status_code = js.status_code\n","    LEFT JOIN contract c ON j.contract_code = c.contract_code\n","    LEFT JOIN action_officer ao ON jsl.allocated_officer = ao.officer_code\n","    LEFT JOIN job_status_log_when_logged jsl_log ON j.job_number = jsl_log.job_number\n","\"\"\")\n","\n","# Display preview for debugging\n","display(jobs_df.limit(10))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"432cfabd-4ce9-4444-9882-18d0e16ef6a4"},{"cell_type":"code","source":["# All Measurements Table Transformation\n","\n","logger.info(\"Creating All Measurements table\")\n","\n","# Since EFF_ST_DT is no longer available, we'll simply get all measurements\n","# If you need to identify the \"latest\" measurement in the future, you'll need to\n","# determine which column in your new schema indicates recency/version\n","\n","all_measurements_df = spark.sql(\"\"\"\n","    SELECT \n","        m.site_code, \n","        m.plot_number, \n","        m.measurement_code AS `Measurement code`, \n","        m.feature_quantity AS `Value`,\n","        concat(m.site_code, '-', cast(m.plot_number as string)) AS `Site and plot`\n","    FROM DIMSU_Lakehouse_Bronze.assets.confirm_feat_measurement m\n","    JOIN park_filter pf ON m.site_code = pf.site_code AND m.plot_number = pf.plot_number\n","    WHERE lower(m.measurement_code) LIKE 'pk%'\n","\"\"\")\n","\n","# Display preview for debugging\n","display(all_measurements_df.limit(10))\n","\n","# Note: If you later identify a column that can be used to determine the \"latest\" measurement,\n","# you would use window functions like this:\n","\"\"\"\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import row_number, desc\n","\n","# Example if you find a different date column:\n","# window_spec = Window.partitionBy(\"site_code\", \"plot_number\", \"Measurement code\").orderBy(desc(\"some_date_column\"))\n","# latest_measurements_df = all_measurements_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n","#                                        .filter(col(\"row_num\") == 1) \\\n","#                                        .drop(\"row_num\")\n","\"\"\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"cfe1555e-10b6-4bda-8d78-697a12bcd954"},{"cell_type":"code","source":["# Clean Up Cached DataFrames\n","\n","logger.info(\"Cleaning up cached DataFrames\")\n","\n","# Unpersist reference tables\n","feature_types_df.unpersist()\n","suburbs_df.unpersist()\n","park_names_df.unpersist()\n","obs_type_df.unpersist()\n","obs_param_df.unpersist()\n","obs_param_options_df.unpersist()\n","job_type_df.unpersist()\n","job_status_df.unpersist()\n","\n","# Unpersist filtering DataFrames\n","south_parks_df.unpersist()\n","park_filter_df.unpersist()\n","\n","# Unpersist other intermediate DataFrames\n","route_df.unpersist()\n","batch_df.unpersist()\n","obs_type_with_param_df.unpersist()\n","job_status_log_current_df.unpersist()\n","job_status_log_when_logged_df.unpersist()\n","contractor_df.unpersist()\n","contract_df.unpersist()\n","action_officer_df.unpersist()\n","\n","logger.info(\"All cached DataFrames unpersisted\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"57c2cbed-76af-4c86-ba50-ac2d0d7b4b4c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"7e362a92-8b71-450b-8a64-c02d058707b4","known_lakehouses":[{"id":"7e362a92-8b71-450b-8a64-c02d058707b4"},{"id":"a865bc24-563c-4fbe-ae83-5aba9d6bd490"}],"default_lakehouse_name":"DIMSU_Lakehouse_Bronze","default_lakehouse_workspace_id":"7e55c4c4-6c70-4e58-b21f-057e46dd83f9"}}},"nbformat":4,"nbformat_minor":5}